{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10a8865",
   "metadata": {},
   "source": [
    "### Sparse Regression analysis of Non linear dynamics of granular material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f43693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File reading and processing: \n",
    "#-------------------------------------------------- Read LAMMPS Dump Files ------------------------------------#\n",
    "def read_lammps_dump(filename, columns_range=('id', 'fz')):\n",
    "    \"\"\"\n",
    "    Read LAMMPS dump file and extract data from specified column range.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filename : str\n",
    "        Path to the LAMMPS dump file\n",
    "    columns_range : tuple, optional\n",
    "        Tuple of (start_column, end_column) names to extract\n",
    "        Default is ('id', 'fz')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing:\n",
    "        - 'timesteps': list of timestep values\n",
    "        - 'natoms': list of number of atoms per timestep\n",
    "        - 'box_bounds': list of box boundary arrays per timestep\n",
    "        - 'data': list of DataFrames with selected columns per timestep\n",
    "        - 'all_data': Combined DataFrame with all timesteps (includes 'timestep' column)\n",
    "    \"\"\"\n",
    "    \n",
    "    timesteps = []\n",
    "    natoms_list = []\n",
    "    box_bounds_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Check for timestep\n",
    "        if line == \"ITEM: TIMESTEP\":\n",
    "            timestep = int(lines[i+1].strip())\n",
    "            timesteps.append(timestep)\n",
    "            i += 2\n",
    "            continue\n",
    "        \n",
    "        # Check for number of atoms\n",
    "        if line == \"ITEM: NUMBER OF ATOMS\":\n",
    "            natoms = int(lines[i+1].strip())\n",
    "            natoms_list.append(natoms)\n",
    "            i += 2\n",
    "            continue\n",
    "        \n",
    "        # Check for box bounds\n",
    "        if line.startswith(\"ITEM: BOX BOUNDS\"):\n",
    "            box_bounds = []\n",
    "            for j in range(3):\n",
    "                bounds = [float(x) for x in lines[i+1+j].strip().split()]\n",
    "                box_bounds.append(bounds)\n",
    "            box_bounds_list.append(np.array(box_bounds))\n",
    "            i += 4\n",
    "            continue\n",
    "        \n",
    "        # Check for atoms data\n",
    "        if line.startswith(\"ITEM: ATOMS\"):\n",
    "            # Extract column names from header\n",
    "            header_parts = line.split()\n",
    "            column_names = header_parts[2:]  # Skip \"ITEM:\" and \"ATOMS\"\n",
    "            \n",
    "            # Find indices of start and end columns\n",
    "            start_col = columns_range[0]\n",
    "            end_col = columns_range[1]\n",
    "            \n",
    "            if start_col not in column_names:\n",
    "                raise ValueError(f\"Start column '{start_col}' not found in dump file\")\n",
    "            if end_col not in column_names:\n",
    "                raise ValueError(f\"End column '{end_col}' not found in dump file\")\n",
    "            \n",
    "            start_idx = column_names.index(start_col)\n",
    "            end_idx = column_names.index(end_col) + 1\n",
    "            selected_columns = column_names[start_idx:end_idx]\n",
    "            \n",
    "            # Read atom data\n",
    "            atom_data = []\n",
    "            natoms_current = natoms_list[-1]\n",
    "            for j in range(natoms_current):\n",
    "                atom_line = lines[i+1+j].strip().split()\n",
    "                # Extract only the selected column range\n",
    "                selected_data = [float(x) for x in atom_line[start_idx:end_idx]]\n",
    "                atom_data.append(selected_data)\n",
    "            \n",
    "            # Create DataFrame for this timestep\n",
    "            df = pd.DataFrame(atom_data, columns=selected_columns)\n",
    "            df['timestep'] = timestep  # Add timestep column for tracking\n",
    "            data_list.append(df)\n",
    "            \n",
    "            i += natoms_current + 1\n",
    "            continue\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Combine all timesteps into single DataFrame\n",
    "    # all_data = pd.concat(data_list, ignore_index=True)\n",
    "    \n",
    "    return {\n",
    "        'timesteps': timesteps,\n",
    "        'natoms': natoms_list,\n",
    "        'box_bounds': box_bounds_list,\n",
    "        'data': data_list,\n",
    "    }\n",
    "\n",
    "def read_all_dumps_in_folder(folder_path, columns_range=('id', 'c_pstress[6]'), \n",
    "                               file_pattern='Dump.shear_fixed_Load_1wall.*', save_combined=False,\n",
    "                               output_filename=None):\n",
    "    folder_path = Path(folder_path)\n",
    "    dump_files = sorted(folder_path.glob(file_pattern))\n",
    "    \n",
    "    if len(dump_files) == 0:\n",
    "        print(f\"No files matching pattern '{file_pattern}' found in {folder_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(dump_files)} dump files in {folder_path}\")\n",
    "    print(f\"Reading files...\")\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    for idx, dump_file in enumerate(dump_files):\n",
    "        try:\n",
    "            result = read_lammps_dump(str(dump_file), columns_range=columns_range)\n",
    "            \n",
    "            # result['data'] is a LIST of DataFrames (one per timestep)\n",
    "            # If each file has only one timestep, take the first one\n",
    "            if len(result['data']) > 0:\n",
    "                df = result['data'][0]  # Take first (and likely only) timestep\n",
    "                df = df.copy()\n",
    "                df['filename'] = dump_file.name\n",
    "                df['file_index'] = idx\n",
    "                all_dataframes.append(df)\n",
    "            \n",
    "            if (idx + 1) % 100 == 0 or (idx + 1) == len(dump_files):\n",
    "                print(f\"  Processed {idx + 1}/{len(dump_files)} files...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {dump_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_dataframes) == 0:\n",
    "        print(\"No data could be read from files\")\n",
    "        return None\n",
    "    \n",
    "    return all_dataframes  # Return list of DataFrames (one per file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------- Calculate the Stress Tensor ------------------------------------#\n",
    "# Per file computation of stress tensor #\n",
    "def cauchy_stress_tensor(df, y, kernel_width):\n",
    "    \"\"\"\n",
    "    Compute stress values from dataframe using IRVING-KIRKWOOD FORMULA\n",
    "    1. Kinetic component: from vx, vy, vz\n",
    "    2. Potential component: from fx, fy, fz and particle separations dx, dy, dz\n",
    "    3. Use Gaussian kernel for spatial averaging\n",
    "    4. Return average stress tensor over all evaluation points y\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'y': y, \n",
    "        's_xy': np.zeros(len(y)), \n",
    "        's_xx': np.zeros(len(y)), \n",
    "        's_yy': np.zeros(len(y)), \n",
    "        's_zz': np.zeros(len(y)),\n",
    "        's_yz': np.zeros(len(y)), \n",
    "        's_zx': np.zeros(len(y)), \n",
    "        'pressure': np.zeros(len(y))\n",
    "    }\n",
    "    \n",
    "    particle_volumes = (4/3) * np.pi * df['radius'].values**3\n",
    "    bin_volume = (3 * kernel_width)**3\n",
    "    \n",
    "    for i, y_eval in enumerate(y):\n",
    "        dist = np.abs(df['y'].values - y_eval)\n",
    "        gaussian_weight = np.exp(-0.5 * (dist / kernel_width)**2)\n",
    "        total_weight = np.sum(gaussian_weight)\n",
    "        \n",
    "        if total_weight < 1e-10:\n",
    "            continue\n",
    "            \n",
    "        # Local average velocities\n",
    "        vx_local = np.sum(df['vx'].values * gaussian_weight) / total_weight\n",
    "        vy_local = np.sum(df['vy'].values * gaussian_weight) / total_weight\n",
    "        vz_local = np.sum(df['vz'].values * gaussian_weight) / total_weight\n",
    "        \n",
    "        # Fluctuation velocities\n",
    "        wx = df['vx'].values - vx_local\n",
    "        wy = df['vy'].values - vy_local\n",
    "        wz = df['vz'].values - vz_local\n",
    "\n",
    "        avg_neighbor_distance = 2 * df['radius'].values  # approximation\n",
    "        dx = avg_neighbor_distance * np.sign(df['fx'].values)\n",
    "        dy = avg_neighbor_distance * np.sign(df['fy'].values)\n",
    "        dz = avg_neighbor_distance * np.sign(df['fz'].values)\n",
    "        \n",
    "        #  =============== IRVING-KIRKWOOD FORMULA ====================\n",
    "        # Kinetic component: w (*) w (tensor product of fluctuation velocities)\n",
    "        # Potential component: f (*) r (tensor product of force and displacement)\n",
    "        \n",
    "        # All stress tensor components\n",
    "        shear_xx = (wx * wx + (df['fx'].values * dx)/2) * particle_volumes / bin_volume\n",
    "        shear_yy = (wy * wy + (df['fy'].values * dy)/2) * particle_volumes / bin_volume\n",
    "        shear_zz = (wz * wz + (df['fz'].values * dz)/2) * particle_volumes / bin_volume\n",
    "        \n",
    "        shear_xy = (wx * wy + (df['fx'].values * dy)/2) * particle_volumes / bin_volume\n",
    "        shear_xz = (wx * wz + (df['fx'].values * dz)/2) * particle_volumes / bin_volume\n",
    "        shear_yz = (wy * wz + (df['fy'].values * dz)/2) * particle_volumes / bin_volume\n",
    "        # =================================================================\n",
    "        \n",
    "        # Weighted average with Gaussian kernel\n",
    "        sxx = np.sum(shear_xx * gaussian_weight) / total_weight\n",
    "        syy = np.sum(shear_yy * gaussian_weight) / total_weight\n",
    "        szz = np.sum(shear_zz * gaussian_weight) / total_weight\n",
    "        \n",
    "        sxy = np.sum(shear_xy * gaussian_weight) / total_weight\n",
    "        sxz = np.sum(shear_xz * gaussian_weight) / total_weight\n",
    "        syz = np.sum(shear_yz * gaussian_weight) / total_weight\n",
    "        \n",
    "        # Stress tensor is symmetric\n",
    "        pressure = (sxx + syy + szz) / 3\n",
    "        \n",
    "        results['s_xx'][i] = sxx\n",
    "        results['s_yy'][i] = syy\n",
    "        results['s_zz'][i] = szz\n",
    "        results['s_xy'][i] = sxy\n",
    "        results['s_yz'][i] = syz\n",
    "        results['s_zx'][i] = sxz\n",
    "        results['pressure'][i] = pressure\n",
    "    \n",
    "    # Average stress tensor components over all evaluation points\n",
    "    a_xx = np.mean(results['s_xx'])\n",
    "    a_yy = np.mean(results['s_yy'])\n",
    "    a_zz = np.mean(results['s_zz'])\n",
    "    a_xy = np.mean(results['s_xy'])\n",
    "    a_yz = np.mean(results['s_yz'])\n",
    "    a_zx = np.mean(results['s_zx'])\n",
    "    \n",
    "\n",
    "    stress_tensor = torch.tensor([\n",
    "        [a_xx, a_xy, a_zx],\n",
    "        [a_xy, a_yy, a_yz],\n",
    "        [a_zx, a_yz, a_zz]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    return stress_tensor\n",
    "\n",
    "#-------------------------------------------------- Calculate the Deformation Tensor ------------------------------------#\n",
    "# Per file computation of stress tensor #\n",
    "def deformation_tensor(df, y, kernel_width):\n",
    "    \"\"\"\n",
    "    Simplified deformation calculation using weighted least s quares.\n",
    "    \"\"\"\n",
    "    shear_rates = []\n",
    "    p1 = 9\n",
    "    p2 = 133\n",
    "    p3 = 475\n",
    "    p4 = 7571.4\n",
    "    Pressure = 0.0\n",
    "    min_particles = 10  # Minimum particles required for calculation\n",
    "    for y_eval in y:\n",
    "        # Select particles near evaluation point\n",
    "        dist = np.abs(df['y'].values - y_eval)\n",
    "        mask = dist < 3 * kernel_width\n",
    "        \n",
    "        if np.sum(mask) < min_particles:\n",
    "            continue\n",
    "        \n",
    "        y_local = df['y'].values[mask]\n",
    "        vx_local = df['vx'].values[mask]\n",
    "        \n",
    "        # Calculate weights\n",
    "        weights = np.exp(-0.5 * (dist[mask] / kernel_width) ** 2)\n",
    "        total_weight = np.sum(weights)\n",
    "        \n",
    "        if total_weight < 1e-20:\n",
    "            continue\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights_norm = weights / total_weight\n",
    "        \n",
    "        # Weighted means\n",
    "        y_mean = np.sum(weights_norm * y_local)\n",
    "        vx_mean = np.sum(weights_norm * vx_local)\n",
    "        \n",
    "        # Weighted covariance and variance\n",
    "        y_centered = y_local - y_mean\n",
    "        vx_centered = vx_local - vx_mean\n",
    "        \n",
    "        cov_y_vx = np.sum(weights_norm * y_centered * vx_centered)\n",
    "        var_y = np.sum(weights_norm * y_centered ** 2)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if var_y < 1e-20:\n",
    "            continue\n",
    "        \n",
    "        # Shear rate = d(vx)/dy ≈ cov(y, vx) / var(y)\n",
    "        shear_rate = cov_y_vx / var_y\n",
    "\n",
    "        # Sanity check\n",
    "        if np.isfinite(shear_rate) and np.abs(shear_rate) < 1e6:\n",
    "            shear_rates.append(shear_rate)\n",
    "    deformation_tensor = torch.tensor([[0.0, np.mean(shear_rates) if len(shear_rates) > 0 else 0.0, 0.0],\n",
    "                                    [np.mean(shear_rates) if len(shear_rates) > 0 else 0.0, 0.0, 0.0],\n",
    "                                    [0.0, 0.0, 0.0]], dtype=torch.float32)\n",
    "    return deformation_tensor\n",
    "\n",
    "def velocity_profile_3d(df, y, kernel_width):\n",
    "    \"\"\"\n",
    "    Compute velocity profile v(y) = [vx(y), vy(y), vz(y)] using Gaussian kernel.\n",
    "    \n",
    "    Returns:\n",
    "        vx_profile, vy_profile, vz_profile: numpy arrays of shape (len(y),)\n",
    "    \"\"\"\n",
    "    min_particles = 10\n",
    "    n_points = len(y)\n",
    "    \n",
    "    vx_profile = np.zeros(n_points)\n",
    "    vy_profile = np.zeros(n_points)\n",
    "    vz_profile = np.zeros(n_points)\n",
    "    \n",
    "    for idx, y_eval in enumerate(y):\n",
    "        # Select particles near evaluation point\n",
    "        dist = np.abs(df['y'].values - y_eval)\n",
    "        mask = dist < 3 * kernel_width\n",
    "        \n",
    "        if np.sum(mask) < min_particles:\n",
    "            vx_profile[idx] = np.nan\n",
    "            vy_profile[idx] = np.nan\n",
    "            vz_profile[idx] = np.nan\n",
    "            continue\n",
    "        \n",
    "        # Get local velocities\n",
    "        vx_local = df['vx'].values[mask]\n",
    "        vy_local = df['vy'].values[mask]\n",
    "        vz_local = df['vz'].values[mask]\n",
    "        \n",
    "        # Calculate Gaussian weights\n",
    "        weights = np.exp(-0.5 * (dist[mask] / kernel_width) ** 2)\n",
    "        total_weight = np.sum(weights)\n",
    "        \n",
    "        if total_weight < 1e-20:\n",
    "            vx_profile[idx] = np.nan\n",
    "            vy_profile[idx] = np.nan\n",
    "            vz_profile[idx] = np.nan\n",
    "            continue\n",
    "        \n",
    "        # Normalize weights and compute weighted mean\n",
    "        weights_norm = weights / total_weight\n",
    "        \n",
    "        vx_profile[idx] = np.sum(weights_norm * vx_local)\n",
    "        vy_profile[idx] = np.sum(weights_norm * vy_local)\n",
    "        vz_profile[idx] = np.sum(weights_norm * vz_local)\n",
    "    \n",
    "    return vx_profile, vy_profile, vz_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2fbad",
   "metadata": {},
   "source": [
    "#### Main processing loop\n",
    "* Reading files\n",
    "* Computing Stress and Deformation \n",
    "* Saving the tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2db986",
   "metadata": {},
   "source": [
    "### Importing Libraries...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620444c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "def Target_matrix(stress, deformation, dt):\n",
    "\n",
    "    # Target matrix: time derivative of each components of stress: x,y,z and shear stress as well xy,yz,zx\n",
    "    n_samples = stress.shape[0]\n",
    "    \n",
    "    target_matrix = torch.zeros(n_samples,6)\n",
    "\n",
    "    tau_xy = torch.zeros(n_samples,1)\n",
    "    \n",
    "    # Compute time derivatives for all samples at once\n",
    "    stress_dot = torch.zeros_like(target_matrix)\n",
    "    stressXX = stress[:,0,0]\n",
    "    stressYY = stress[:,1,1]\n",
    "    stressZZ = stress[:,2,2]\n",
    "    stressXY = stress[:,0,1]\n",
    "    stressYZ = stress[:,1,2]\n",
    "    stressZX = stress[:,2,0]\n",
    "\n",
    "    Stress = torch.stack((stressXX,stressYY,stressZZ,stressXY,stressYZ,stressZX),dim=1)\n",
    "    stress_dot[0] = (Stress[1] - Stress[0]) / dt\n",
    "    stress_dot[-1] = (Stress[-1] - Stress[-2]) / dt\n",
    "    stress_dot[1:-1] = (Stress[2:] - Stress[:-2]) / (2 * dt)\n",
    "    # Convert stress_dot to full 3x3 tensor form\n",
    "    stress_dot_master = torch.zeros((n_samples,3,3), dtype=torch.float32)\n",
    "    stress_dot_master[:,0,0] = stress_dot[:,0]\n",
    "    stress_dot_master[:,1,1] = stress_dot[:,1]\n",
    "    stress_dot_master[:,2,2] = stress_dot[:,2]\n",
    "    stress_dot_master[:,0,1] = stress_dot[:,3]\n",
    "    stress_dot_master[:,1,0] = stress_dot[:,3]\n",
    "    stress_dot_master[:,1,2] = stress_dot[:,4]\n",
    "    stress_dot_master[:,2,1] = stress_dot[:,4]\n",
    "    stress_dot_master[:,2,0] = stress_dot[:,5]\n",
    "    stress_dot_master[:,0,2] = stress_dot[:,5]\n",
    "\n",
    "    # Build W tensors for all samples\n",
    "    W = torch.zeros_like(stress)\n",
    "    shear_rates = deformation[:, 0, 1]  # Extract all shear rates\n",
    "    W[:, 0, 1] = 0.5 * shear_rates\n",
    "    W[:, 1, 0] = -0.5 * shear_rates\n",
    "    \n",
    "    # Compute Jaumann derivative for all samples\n",
    "    j_t = stress_dot_master - torch.bmm(W, stress) + torch.bmm(stress, W)\n",
    "\n",
    "    # Populating target matrix\n",
    "    target_matrix[:,0] = j_t[:,0,0]\n",
    "    target_matrix[:,1] = j_t[:,1,1]\n",
    "    target_matrix[:,2] = j_t[:,2,2]\n",
    "    target_matrix[:,3] = j_t[:,0,1] # tau_xy\n",
    "    # tau_xy[:, 0] = stress[:,0,1]\n",
    "    tau_xy[:,0] = j_t[:,0,1]\n",
    "    target_matrix[:,4] = j_t[:,1,2]\n",
    "    target_matrix[:,5] = j_t[:,2,0]\n",
    "    #return target_matrix\n",
    "    return tau_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a414a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def granular_library(stress, deformation, velocity_profiles, dX):\n",
    "    \"\"\"\n",
    "    Library of candidate terms for the stress-deformation relationship.\n",
    "    Arguments:\n",
    "        stress: torch.Tensor of shape [n_samples, 3, 3] - stress tensor over time\n",
    "        deformation: torch.Tensor of shape [n_samples, 3, 3] - deformation tensor over time\n",
    "        velocity_profiles: torch.Tensor of shape [n_samples, n_y_points, 3] - velocity at spatial points\n",
    "        p: reference pressure for non-dimensionalization\n",
    "        dX: spatial discretization (particle diameter or kernel width)\n",
    "    Returns:\n",
    "        Library: torch.Tensor of shape [n_samples, 26], 26 terms: unity, stress (all 6 independent components), deformation, laplacian of Deformation tensor, and granular temparature\n",
    "        LIBRARY_TERMS: dict mapping term names to column indices\n",
    "    \"\"\"\n",
    "    n_samples = deformation.shape[0]  # time steps\n",
    "    n_y_points = velocity_profiles.shape[1]  # spatial evaluation points\n",
    "    \n",
    "    LIBRARY_TERMS = {\n",
    "        '1': 0,\n",
    "        'tau_xx': 1, 'tau_yy': 2, 'tau_zz': 3, \n",
    "        'tau_xy': 4, 'tau_yz': 5, 'tau_zx': 6,\n",
    "        'D_xx': 7, 'D_yy': 8, 'D_zz': 9, \n",
    "        'D_xy': 10, 'D_yz': 11, 'D_zx': 12,\n",
    "        'D_xx^2': 13, 'D_yy^2': 14, 'D_zz^2': 15, \n",
    "        'D_xy^2': 16, 'D_yz^2': 17, 'D_zx^2': 18,\n",
    "        'lap_D_xx': 19, 'lap_D_yy': 20, 'lap_D_zz': 21, \n",
    "        'lap_D_xy': 22, 'lap_D_yz': 23, 'lap_D_zx': 24, \n",
    "        'T': 25, 'phi':26, 'lap_phi':27\n",
    "    }\n",
    "    lib_xy = {\n",
    "        'tau_xy':0, 'D_xy':1, 'D_xy^2':2, 'lap_D_xy':3, 'Tg':4\n",
    "    }\n",
    "    \n",
    "    Library = torch.zeros((n_samples, 26), dtype=torch.float32)\n",
    "    theta = torch.zeros((n_samples, 5), dtype=torch.float32)\n",
    "    # ==================== STRESS TERMS (Non-dimensionalized) ====================\n",
    "    pressure = (stress[:, 0, 0] + stress[:, 1, 1] + stress[:, 2, 2]) / 3.0 + 1e-10\n",
    "    \n",
    "    tau_xx = stress[:, 0, 0] \n",
    "    tau_yy = stress[:, 1, 1]\n",
    "    tau_zz = stress[:, 2, 2]\n",
    "    tau_xy = stress[:, 0, 1]\n",
    "    tau_yz = stress[:, 1, 2]\n",
    "    tau_zx = stress[:, 2, 0]\n",
    "    \n",
    "    # ==================== DEFORMATION TERMS ====================\n",
    "    D_xx = deformation[:, 0, 0]\n",
    "    D_yy = deformation[:, 1, 1]\n",
    "    D_zz = deformation[:, 2, 2]\n",
    "    D_xy = deformation[:, 0, 1]\n",
    "    D_yz = deformation[:, 1, 2]\n",
    "    D_zx = deformation[:, 2, 0]\n",
    "    \n",
    "    # ==================== LAPLACIAN OF DEFORMATION (Temporal) ====================\n",
    "    # Compute laplacian(D) using second-order central differences\n",
    "    h2 = dX * dX\n",
    "    \n",
    "    def laplacian(component,h2):\n",
    "        \"\"\"\n",
    "        Using central difference: (D[i+1] - 2*D[i] + D[i-1]) / dx*dx\n",
    "        \"\"\"\n",
    "        lap = torch.zeros_like(component)\n",
    "        lap[1:-1] = (component[2:] - 2 * component[1:-1] + component[:-2]) / h2\n",
    "        # Boundary: forward/backward difference\n",
    "        lap[0] = (component[1] - component[0]) / h2\n",
    "        lap[-1] = (component[-1] - component[-2]) / h2\n",
    "        return lap\n",
    "    \n",
    "    lap_D_xx = laplacian(D_xx, h2)\n",
    "    lap_D_yy = laplacian(D_yy, h2)\n",
    "    lap_D_zz = laplacian(D_zz, h2)\n",
    "    lap_D_xy = laplacian(D_xy, h2)\n",
    "    lap_D_yz = laplacian(D_yz, h2)\n",
    "    lap_D_zx = laplacian(D_zx, h2)\n",
    "    \n",
    "    # ==================== GRANULAR TEMPERATURE ====================\n",
    "    # T_g = (1/3) * <(v - <v>)^2> averaged over space and velocity components\n",
    "    # Shape of velocity_profiles: [n_samples, n_y_points, 3] for (vx, vy, vz)\n",
    "    \n",
    "    T_g = torch.zeros(n_samples, dtype=torch.float32)\n",
    "    \n",
    "    for t in range(n_samples):\n",
    "        # Get velocity field at time t: shape [n_y_points, 3]\n",
    "        v_field = velocity_profiles[t]  # [n_y_points, 3]\n",
    "        \n",
    "        # Spatial average of velocity at each component\n",
    "        v_mean = torch.mean(v_field, dim=0, keepdim=True)  # [1, 3]\n",
    "        \n",
    "        # Fluctuation velocities\n",
    "        v_fluctuation = v_field - v_mean  # [n_y_points, 3]\n",
    "        \n",
    "        # Squared fluctuation velocity magnitude\n",
    "        v_sq_fluct = torch.sum(v_fluctuation**2, dim=1)  # [n_y_points]\n",
    "        \n",
    "        # Granular temperature: (1/3) * spatial average of |v'|²\n",
    "        T_g[t] = torch.mean(v_sq_fluct) / 3.0\n",
    "    \n",
    "    # Non-dimensionalize by characteristic velocity squared\n",
    "    # For quasi-static flows, use: v_char² = p / ρ (if available)\n",
    "    # Or normalize by maximum observed temperature\n",
    "    T_g_normalized = T_g #/ (torch.max(T_g) + 1e-10)\n",
    "\n",
    "    # ====================== local Volume fraction ==================== #\n",
    "    \n",
    "\n",
    "    \n",
    "    # ==================== POPULATE LIBRARY MATRIX ==================== #\n",
    "    Library[:, LIBRARY_TERMS['1']] = torch.ones(n_samples, dtype=torch.float32)\n",
    "    # Stress terms\n",
    "    Library[:, LIBRARY_TERMS['tau_xx']] = tau_xx\n",
    "    Library[:, LIBRARY_TERMS['tau_yy']] = tau_yy\n",
    "    Library[:, LIBRARY_TERMS['tau_zz']] = tau_zz\n",
    "    Library[:, LIBRARY_TERMS['tau_xy']] = tau_xy\n",
    "    Library[:, LIBRARY_TERMS['tau_yz']] = tau_yz\n",
    "    Library[:, LIBRARY_TERMS['tau_zx']] = tau_zx\n",
    "    # Linear deformation terms\n",
    "    Library[:, LIBRARY_TERMS['D_xx']] = D_xx\n",
    "    Library[:, LIBRARY_TERMS['D_yy']] = D_yy\n",
    "    Library[:, LIBRARY_TERMS['D_zz']] = D_zz\n",
    "    Library[:, LIBRARY_TERMS['D_xy']] = D_xy\n",
    "    Library[:, LIBRARY_TERMS['D_yz']] = D_yz\n",
    "    Library[:, LIBRARY_TERMS['D_zx']] = D_zx\n",
    "    # Squared deformation terms\n",
    "    Library[:, LIBRARY_TERMS['D_xx^2']] = (D_xx**2)\n",
    "    Library[:, LIBRARY_TERMS['D_yy^2']] = (D_yy**2)\n",
    "    Library[:, LIBRARY_TERMS['D_zz^2']] = (D_zz**2)\n",
    "    Library[:, LIBRARY_TERMS['D_xy^2']] = (D_xy**2)\n",
    "    Library[:, LIBRARY_TERMS['D_yz^2']] = (D_yz**2)\n",
    "    Library[:, LIBRARY_TERMS['D_zx^2']] = (D_zx**2)\n",
    "    # Laplacian terms\n",
    "    Library[:, LIBRARY_TERMS['lap_D_xx']] = lap_D_xx\n",
    "    Library[:, LIBRARY_TERMS['lap_D_yy']] = lap_D_yy\n",
    "    Library[:, LIBRARY_TERMS['lap_D_zz']] = lap_D_zz\n",
    "    Library[:, LIBRARY_TERMS['lap_D_xy']] = lap_D_xy\n",
    "    Library[:, LIBRARY_TERMS['lap_D_yz']] = lap_D_yz\n",
    "    Library[:, LIBRARY_TERMS['lap_D_zx']] = lap_D_zx\n",
    "    # Granular temperature\n",
    "    Library[:, LIBRARY_TERMS['T']] = T_g_normalized\n",
    "\n",
    "    # Library only for xy component\n",
    "    theta[:, lib_xy['tau_xy']] = tau_xy\n",
    "    theta[:, lib_xy['D_xy']] = D_xy\n",
    "    theta[:, lib_xy['D_xy^2']] = (D_xy**2)\n",
    "    theta[:, lib_xy['lap_D_xy']] = lap_D_xy\n",
    "    theta[:, lib_xy['Tg']] = T_g_normalized\n",
    "\n",
    "    #return Library, LIBRARY_TERMS\n",
    "    return theta, lib_xy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def STRidge_optimizer(target_matrix, library_matrix, lam, iterations=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    STRidge optimizer for sparse regression\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_matrix : torch.Tensor, shape [n_samples, 6] - flattened stress derivatives\n",
    "    library_matrix : torch.Tensor, shape [n_samples, n_terms] - flattened library terms\n",
    "    lam : float - regularization parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    coefficients : torch.Tensor, shape [n_terms, 6] - one coefficient vector per stress component\n",
    "    \"\"\"\n",
    "    n_samples, n_terms = library_matrix.shape\n",
    "    n_components = target_matrix.shape[1]  # Should be 6 or 1 for tau_xy\n",
    "    \n",
    "    # Initialize coefficients for each stress component\n",
    "    torch.manual_seed(42)\n",
    "    coefficients = torch.randn(n_terms, n_components, requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([coefficients], lr=0.01)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predicted: [n_samples, n_terms] @ [n_terms, 1] -> [n_samples, 1]\n",
    "        predicted = torch.mm(library_matrix, coefficients)\n",
    "        \n",
    "        # Ridge loss\n",
    "        loss = torch.mean((predicted - target_matrix) ** 2) + lam * torch.sum(coefficients ** 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Thresholding step\n",
    "        with torch.no_grad():\n",
    "            threshold = lam * 0.1  # Adaptive threshold\n",
    "            coefficients[torch.abs(coefficients) < threshold] = 0.0\n",
    "        \n",
    "        # Check convergence\n",
    "        if iteration > 10 and loss.item() < tol:\n",
    "            break\n",
    "    \n",
    "    return coefficients.detach()\n",
    "\n",
    "\n",
    "def optimal_matrix(target_tensor, library_tensor, lam, iterations=20, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Optimized sparse regression with train-test split\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_tensor : torch.Tensor, shape [n_samples, 6] - Jaumann stress derivatives\n",
    "    library_tensor : torch.Tensor, shape [n_samples, n_terms, 3, 3] - Library terms\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xhi_best : torch.Tensor, shape [n_terms, 6] - Best coefficients\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples, n_terms= library_tensor.shape\n",
    " \n",
    "    # Split data\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        np.arange(n_samples), test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_target = target_tensor[train_idx]\n",
    "    test_target = target_tensor[test_idx]\n",
    "    train_library = library_tensor[train_idx]\n",
    "    test_library = library_tensor[test_idx]\n",
    "    \n",
    "    # Initialize\n",
    "    err_best = float('inf')\n",
    "    xhi_best = None\n",
    "    tol_current = tol\n",
    "    d_tol = tol / 10\n",
    "    \n",
    "    # Initial guess using least squares\n",
    "    try:\n",
    "        # Solve: train_library @ xhi = train_target\n",
    "        xhi_initial = torch.linalg.lstsq(train_library, train_target).solution\n",
    "    except:\n",
    "        xhi_initial = torch.zeros(n_terms, 6)\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        # Get coefficients using STRidge\n",
    "        w = STRidge_optimizer(train_target, train_library, lam, iterations=100, tol=tol_current)\n",
    "        \n",
    "        # Compute test error\n",
    "        predicted_test = torch.mm(test_library, w)\n",
    "        err = torch.norm(test_target - predicted_test).item()\n",
    "        l0_penalty = 0.001 * torch.count_nonzero(w).item()\n",
    "        total_err = err + l0_penalty\n",
    "        \n",
    "        # Update best\n",
    "        if total_err < err_best:\n",
    "            err_best = total_err\n",
    "            xhi_best = w\n",
    "            tol_current = tol_current + d_tol\n",
    "        else:\n",
    "            tol_current = max(0, tol_current - 2 * d_tol)\n",
    "            d_tol = 2 * d_tol / (iterations - iter + 1)\n",
    "            tol_current = tol_current + d_tol\n",
    "        \n",
    "        # print(f\"Iteration {iter+1}/{iterations}: Error = {total_err:.6f}, L0 = {torch.count_nonzero(w).item()}\")\n",
    "    \n",
    "    return xhi_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79cdfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_equations(library_terms, xi):\n",
    "    n_equations = xi.shape[1]  # 6 equations\n",
    "    library = [''] * len(library_terms)\n",
    "    for term, idx in library_terms.items():\n",
    "        library[idx] = term\n",
    "    for j in range(n_equations):\n",
    "        terms = []\n",
    "        for i in range(len(library)):\n",
    "            coef = xi[i, j]\n",
    "            if abs(coef) > 1e-6: \n",
    "                if coef >= 0 and terms:\n",
    "                    terms.append(f\"+ {coef:.6f}*{library[i]}\")\n",
    "                else:\n",
    "                    terms.append(f\"{coef:.6f}*{library[i]}\")\n",
    "        \n",
    "        print(f\"σ_{j+1} = \" + \" \".join(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a815bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "vol_frac = ['578', '588', '601']  # 3 volume fractions\n",
    "vt = ['0.0031', '0.014', '0.031', '0.085', '0.14', '0.23']  # 6 velocities\n",
    "all_combinations = list(product(vol_frac, vt))\n",
    "random.seed(1)\n",
    "random.shuffle(all_combinations)\n",
    "\n",
    "y_eval = np.linspace(-0.00004, 0.4, 100)\n",
    "kernel_width = 0.02\n",
    "\n",
    "# Pre-allocate 4D tensors\n",
    "n_folders = len(vol_frac) * len(vt)  # 18 folders\n",
    "n_y_points = len(y_eval)  # 100 spatial points\n",
    "\n",
    "xi_coefficients = None\n",
    "folder_idx = 0\n",
    "xi_matrix = None\n",
    "for i, j in all_combinations:\n",
    "    folder_path = Path(r'F:\\DATA_constant_volume_DEM\\DATA_constant_volume') / f'vf{i}_vt{j}'\n",
    "    \n",
    "    print(f\"\\nProcessing folder {folder_idx+1}/{n_folders}: vf{i}_vt{j}\")\n",
    "    if not folder_path.exists():\n",
    "        print(f\"  ⚠ Folder does not exist, skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        result = read_all_dumps_in_folder(\n",
    "            folder_path, \n",
    "            columns_range=('id', 'fz'), \n",
    "            file_pattern='Dump.shear_fixed_Load_1wall.*', \n",
    "            save_combined=False,\n",
    "            output_filename=None\n",
    "        )\n",
    "\n",
    "        n_files = len(result)   #count the total number of files in the folder: time step dimension\n",
    "        stress_data = torch.zeros((n_files, 3, 3), dtype=torch.float32)\n",
    "        deformation_data = torch.zeros((n_files, 3, 3), dtype=torch.float32)\n",
    "        velocity_data = torch.zeros((n_files, len(y_eval), 3), dtype=torch.float32)\n",
    "\n",
    "        print(f\"  Found {len(result)} files\")\n",
    "\n",
    "        for file_idx, dump_df in enumerate(result):\n",
    "            # Get 3×3 stress tensor\n",
    "            stress_3x3 = cauchy_stress_tensor(dump_df, y_eval, kernel_width)\n",
    "            # Get 3×3 deformation tensor\n",
    "            deform_3x3 = deformation_tensor(dump_df, y_eval, kernel_width)\n",
    "            # NEW: Get velocity profile at all y evaluation points\n",
    "            vx_profile, vy_profile, vz_profile = velocity_profile_3d(dump_df, y_eval, kernel_width)\n",
    "\n",
    "            stress_data[file_idx] = stress_3x3\n",
    "            deformation_data[file_idx] = deform_3x3\n",
    "            velocity_data[file_idx, :, 0] = torch.from_numpy(vx_profile)\n",
    "            velocity_data[file_idx, :, 1] = torch.from_numpy(vy_profile)\n",
    "            velocity_data[file_idx, :, 2] = torch.from_numpy(vz_profile)\n",
    "            #####\n",
    "            # Since time steps are different, rather than storing, implement the regression here inside the loop for this will act as the main loop for folders\n",
    "            # Nested loop will have one for stridge regression analysis to come up with optimal coefficients\n",
    "            #####\n",
    "\n",
    "            if (file_idx + 1) % 500 == 0:\n",
    "                print(f\"  Processed {file_idx + 1}/{len(result)} files...\")\n",
    "\n",
    "        # spacial and temporal elements\n",
    "        dx = 0.02\n",
    "        dt = 0.0003\n",
    "        target_Matrix = Target_matrix(stress_data, deformation_data, dt)\n",
    "        Library, LIBRARY_TERMS = granular_library(stress_data, deformation_data, velocity_data, dx)\n",
    "        \n",
    "        print(f\" Target and library matrix created, with sizes (resp): \", target_Matrix.shape, Library.shape)\n",
    "        print(f\"  Running STRidge optimization...\")\n",
    "        xi_coefficients = optimal_matrix(target_Matrix, Library, 0.03, iterations=500, tol=1e-6)\n",
    "\n",
    "\n",
    "        print(f\" Optimal Coefficient obtained: \", xi_coefficients)\n",
    "        print(f\" target matrix: \", target_Matrix.sum())\n",
    "        print(f\"✓ Completed vf{i}_vt{j}\")\n",
    "\n",
    "        folder_idx += 1\n",
    "        \n",
    "print(\"✓ Computation completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27738c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-6\n",
    "sigma_t  = torch.zeros_like(target_Matrix)\n",
    "x,y = target_Matrix.shape\n",
    "for i in range(x):\n",
    "    for j in range(y):\n",
    "        if target_Matrix[i,j] >= tol:\n",
    "            sigma_t[i,j] = target_Matrix[i,j]\n",
    "print(sigma_t.sum(0), ' = ')\n",
    "print_equations(LIBRARY_TERMS, xi_coefficients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_granular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
